{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0906a87b",
   "metadata": {},
   "source": [
    "# Elastic Net Regression\n",
    "\n",
    "## Overview\n",
    "Elastic Net Regression is a **regularized linear regression** technique that combines both the **L1 penalty (Lasso)** and the **L2 penalty (Ridge)** in the loss function. It is particularly useful when there are **correlated features** and when you want both **coefficient shrinkage** and **automatic feature selection**.\n",
    "\n",
    "---\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "The Elastic Net loss function is:\n",
    "\n",
    "$$L = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 + a\\|w\\|^2 + b\\|w\\|$$\n",
    "\n",
    "Where:\n",
    "- $a\\|w\\|^2 = a \\sum_{j=1}^{p} b_j^2$ — **L2 (Ridge)** penalty term\n",
    "- $b\\|w\\| = b \\sum_{j=1}^{p} |b_j|$ — **L1 (Lasso)** penalty term\n",
    "\n",
    "### Relationship to scikit-learn parameters (`alpha` and `l1_ratio`):\n",
    "\n",
    "$$\\lambda = a + b \\qquad \\text{(total regularization strength = alpha)}$$\n",
    "\n",
    "$$\\text{l1\\_ratio} = \\frac{a}{a + b} \\qquad \\Rightarrow \\qquad l1 = \\frac{a}{\\lambda}$$\n",
    "\n",
    "$$a = \\text{l1\\_ratio} \\times \\lambda \\qquad b = \\lambda - a$$\n",
    "\n",
    "| Parameter | Meaning | Example ($\\lambda=1$, $\\text{l1\\_ratio}=0.5$) |\n",
    "|-----------|---------|------------------------------------------------|\n",
    "| $\\lambda$ (`alpha`) | Total regularization strength | $1$ |\n",
    "| $\\text{l1\\_ratio}$ | Fraction of L1 (Lasso) | $0.5$ |\n",
    "| $a$ | L1 penalty coefficient | $0.5$ |\n",
    "| $b$ | L2 penalty coefficient | $0.5$ |\n",
    "\n",
    "> **Example:** `l1_ratio=0.9` → $a = 0.9\\lambda$ (90% Lasso), $b = 0.1\\lambda$ (10% Ridge)\n",
    "\n",
    "### Effect of `l1_ratio` on model type:\n",
    "\n",
    "| `l1_ratio` | $a$ (L1) | $b$ (L2) | Equivalent Model |\n",
    "|------------|----------|----------|-----------------|\n",
    "| `0` | $0$ | $\\lambda$ | **Pure Ridge** (only L2 penalty) |\n",
    "| `0 < r < 1` | $r\\lambda$ | $(1-r)\\lambda$ | **Elastic Net** (blend of both) |\n",
    "| `1` | $\\lambda$ | $0$ | **Pure Lasso** (only L1 penalty) |\n",
    "\n",
    "The intercept (bias term $b_0$) is **not penalized** — only slope coefficients are shrunk.\n",
    "\n",
    "---\n",
    "\n",
    "## Closed-Form Solution\n",
    "\n",
    "Elastic Net does **not have a closed-form solution** due to the non-differentiability of the L1 component. It is solved iteratively using:\n",
    "- **Coordinate Descent** (default in scikit-learn)\n",
    "- **Pathwise Coordinate Descent** (efficient for a sequence of alpha values)\n",
    "\n",
    "---\n",
    "\n",
    "## Key Properties\n",
    "\n",
    "- Elastic Net **shrinks** coefficients toward zero and can set some coefficients **exactly to zero** (like Lasso)\n",
    "- The L2 component groups correlated features together (unlike Lasso which arbitrarily picks one)\n",
    "- When $\\text{l1\\_ratio} = 1$ — Elastic Net reduces to **Lasso** ($b = 0$, pure L1)\n",
    "- When $\\text{l1\\_ratio} = 0$ — Elastic Net reduces to **Ridge** ($a = 0$, pure L2)\n",
    "- When $\\lambda = 0$ — Elastic Net reduces to **Ordinary Least Squares (OLS)**\n",
    "- Larger $\\lambda$ = more regularization = sparser model = higher bias, lower variance\n",
    "\n",
    "---\n",
    "\n",
    "## Impact on Bias and Variance\n",
    "\n",
    "As $\\lambda$ increases:\n",
    "\n",
    "| Metric | Effect |\n",
    "|--------|--------|\n",
    "| Bias | Increases |\n",
    "| Variance | Decreases |\n",
    "| Non-zero Coefficients | Decreases (more sparsity as $\\text{l1\\_ratio} \\to 1$) |\n",
    "| Total Loss | Decreases initially, then increases |\n",
    "\n",
    "The optimal $\\lambda$ and $\\text{l1\\_ratio}$ are found at the **sweet spot** where total loss is minimized — typically via cross-validation.\n",
    "\n",
    "---\n",
    "\n",
    "## Geometric Interpretation (Constraint Region)\n",
    "\n",
    "In coefficient space ($\\beta_1$, $\\beta_2$):\n",
    "- OLS minimizes MSE — represented by **elliptical contours**\n",
    "- Elastic Net adds a constraint region that is a **blend of a circle (Ridge) and a diamond (Lasso)**\n",
    "- The constraint region has **rounded corners** — more corner-like than Ridge (can zero out coefficients) but smoother than Lasso (handles correlated features better)\n",
    "- The solution is where the **ellipse first touches the blended region**\n",
    "\n",
    "---\n",
    "\n",
    "## Elastic Net vs Ridge vs Lasso\n",
    "\n",
    "| Property | Ridge (L2) | Lasso (L1) | Elastic Net (L1 + L2) |\n",
    "|----------|-----------|-----------|----------------------|\n",
    "| Penalty | $b\\|w\\|^2$ | $a\\|w\\|$ | $a\\|w\\| + b\\|w\\|^2$ |\n",
    "| Coefficients reach zero | Never | Yes | Yes |\n",
    "| Feature Selection | No | Yes | Yes |\n",
    "| Handles correlated features | Yes | Picks one | Yes (groups them) |\n",
    "| Constraint Region | Circle | Diamond | Rounded Diamond |\n",
    "| Best for | Multicollinearity | Sparse features | Correlated + sparse features |\n",
    "\n",
    "---\n",
    "\n",
    "## Syntax (Scikit-learn)\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "en = ElasticNet(alpha=1.0, l1_ratio=0.5)   # alpha = λ = a+b,  l1_ratio = a/(a+b)\n",
    "en.fit(X_train, y_train)\n",
    "y_pred = en.predict(X_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## When to Use Elastic Net Regression\n",
    "\n",
    "| Condition | Use Elastic Net? |\n",
    "|-----------|-----------------|\n",
    "| Correlated features present | Yes |\n",
    "| Need feature selection | Yes |\n",
    "| All features are likely relevant | No (use Ridge) |\n",
    "| Sparse features with no correlation | No (use Lasso) |\n",
    "| Model is overfitting | Yes |\n",
    "| Unsure between Ridge and Lasso | Yes |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e28874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284ccf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_diabetes()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "print(\"Feature shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n",
    "print(\"\\nFeature names:\", data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "799d9080",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f508dc8",
   "metadata": {},
   "source": [
    "## Model Comparison: Linear Regression, Ridge, Lasso, and Elastic Net\n",
    "\n",
    "We compare all four models on the same **Diabetes dataset** to observe the impact of different regularization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a242b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4399387660024645"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear Regression (Baseline)\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "print(\"=== Linear Regression ===\")\n",
    "print(\"R² Score:\", round(r2_score(y_test, y_pred), 4))\n",
    "print(\"MSE:     \", round(mean_squared_error(y_test, y_pred), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41eb10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4519973816947852"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridge Regression (L2 penalty)\n",
    "reg = Ridge(alpha=0.1)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "print(\"=== Ridge Regression (alpha=0.1) ===\")\n",
    "print(\"R² Score:\", round(r2_score(y_test, y_pred), 4))\n",
    "print(\"MSE:     \", round(mean_squared_error(y_test, y_pred), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b53e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4411227990495632"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lasso Regression (L1 penalty)\n",
    "reg = Lasso(alpha=0.01)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "print(\"=== Lasso Regression (alpha=0.01) ===\")\n",
    "print(\"R² Score:\", round(r2_score(y_test, y_pred), 4))\n",
    "print(\"MSE:     \", round(mean_squared_error(y_test, y_pred), 4))\n",
    "print(\"Zero coefficients:\", (reg.coef_ == 0).sum(), \"out of\", len(reg.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc65b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4531493801165679"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Elastic Net Regression (L1 + L2 penalty)\n",
    "reg = ElasticNet(alpha=0.005, l1_ratio=0.9)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "print(\"=== Elastic Net Regression (alpha=0.005, l1_ratio=0.9) ===\")\n",
    "print(\"R² Score:\", round(r2_score(y_test, y_pred), 4))\n",
    "print(\"MSE:     \", round(mean_squared_error(y_test, y_pred), 4))\n",
    "print(\"Zero coefficients:\", (reg.coef_ == 0).sum(), \"out of\", len(reg.coef_))\n",
    "print(\"Coefficients:\", np.round(reg.coef_, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782a8f68",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "Comparing all four models on the Diabetes dataset reveals how regularization affects performance and sparsity:\n",
    "\n",
    "- Linear Regression serves as the baseline — no regularization, can overfit on noisy data\n",
    "- Ridge slightly adjusts coefficients but keeps all features — useful when all features matter\n",
    "- Lasso may zero out some coefficients — useful for feature selection\n",
    "- Elastic Net (with `l1_ratio=0.9`) leans heavily toward Lasso but uses Ridge to stabilize correlated features\n",
    "\n",
    "### Why Similar Performance Here?\n",
    "\n",
    "| Factor | Impact |\n",
    "|--------|--------|\n",
    "| Small dataset (442 samples) | Regularization differences are more visible |\n",
    "| Only 10 features | Limited room for feature selection to show huge gains |\n",
    "| Low $\\lambda$ values | Weak regularization — all models behave close to OLS |\n",
    "| No strong correlation | Elastic Net's grouping benefit is less visible |\n",
    "\n",
    "**Takeaway:** Elastic Net shines on **high-dimensional datasets** with **correlated features** — it combines Ridge's ability to handle collinearity with Lasso's feature elimination, giving the best of both worlds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af6f418",
   "metadata": {},
   "source": [
    "## Finding the Optimal Parameters ($\\lambda$ and `l1_ratio`)\n",
    "\n",
    "**Using ElasticNetCV** (Recommended)\n",
    "\n",
    "`ElasticNetCV` performs cross-validation over a grid of both `alpha` and `l1_ratio` values to automatically find the optimal combination. This avoids manual grid search and is much more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb67b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "l1_ratios = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "en_cv = ElasticNetCV(alphas=alphas, l1_ratio=l1_ratios, cv=5, random_state=42, max_iter=10000, n_jobs=-1)\n",
    "en_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Optimal alpha (lambda):\", en_cv.alpha_)\n",
    "print(\"Optimal l1_ratio (rho):\", en_cv.l1_ratio_)\n",
    "\n",
    "y_pred_cv = en_cv.predict(X_test)\n",
    "print(\"\\nElasticNetCV R² Score:\", round(r2_score(y_test, y_pred_cv), 4))\n",
    "print(\"ElasticNetCV MSE:     \", round(mean_squared_error(y_test, y_pred_cv), 4))\n",
    "print(\"Zero coefficients:\", (en_cv.coef_ == 0).sum(), \"out of\", len(en_cv.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492fc50d",
   "metadata": {},
   "source": [
    "### **Can also use SGDRegressor**\n",
    "\n",
    "Stochastic Gradient Descent (SGD) can be used to fit an Elastic Net model by setting `penalty='elasticnet'` and tuning the `l1_ratio` parameter. This is particularly useful for very large datasets where coordinate descent may be slow.\n",
    "\n",
    "- Set `penalty='l2'` for Ridge behavior\n",
    "- Set `penalty='l1'` for Lasso behavior\n",
    "- Set `penalty='elasticnet'` with `l1_ratio` for Elastic Net behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d6f824",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Understandings\n",
    "\n",
    "**1. What Elastic Net does**\n",
    "Combines both the L1 ($\\lambda \\rho \\sum |b_j|$) and L2 ($\\lambda \\frac{1-\\rho}{2} \\sum b_j^2$) penalties in a single loss function. The `l1_ratio` ($\\rho$) controls the blend — closer to 1 means more Lasso-like, closer to 0 means more Ridge-like.\n",
    "\n",
    "**2. Coefficients can reach exactly zero**\n",
    "Like Lasso, Elastic Net can drive coefficients to exactly zero due to the L1 component — performing built-in feature selection. But unlike Lasso, the L2 component prevents indiscriminate elimination of correlated features.\n",
    "\n",
    "**3. Handles correlated features better than Lasso**\n",
    "Lasso tends to arbitrarily pick one feature among a correlated group and zero out the rest. Elastic Net groups correlated features together and retains or shrinks them proportionally — a more stable and interpretable result.\n",
    "\n",
    "**4. Two hyperparameters to tune**\n",
    "Unlike Ridge and Lasso, Elastic Net has two parameters:\n",
    "- `alpha` ($\\lambda$) — overall regularization strength\n",
    "- `l1_ratio` ($\\rho$) — the mix between L1 and L2\n",
    "\n",
    "Use `ElasticNetCV` to find the optimal combination automatically via cross-validation.\n",
    "\n",
    "**5. Bias-Variance tradeoff**\n",
    "As $\\lambda$ increases: variance drops (less overfitting) and the model becomes sparser (more like Lasso when $\\rho$ is high). The `l1_ratio` controls how aggressively coefficients are zeroed out versus just shrunk.\n",
    "\n",
    "**6. No closed-form solution**\n",
    "Because of the absolute value (L1) component, Elastic Net is solved iteratively using **Coordinate Descent**, just like Lasso.\n",
    "\n",
    "**7. Geometric view**\n",
    "Elastic Net's constraint region is a **rounded diamond** — smoother than Lasso's sharp diamond but more angular than Ridge's circle. This means some coefficients can still reach exactly zero (like Lasso), but correlated groups aren't arbitrarily cut (like Ridge).\n",
    "\n",
    "**8. When to use Elastic Net**\n",
    "Use Elastic Net when:\n",
    "- You have many features and some are correlated\n",
    "- You want feature selection but Lasso is too unstable (e.g., picks different features on similar datasets)\n",
    "- You're unsure whether to use Ridge or Lasso — Elastic Net is a robust default\n",
    "- High-dimensional data (e.g., genomics, text classification) with both sparsity and collinearity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
